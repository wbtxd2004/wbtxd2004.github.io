---
layout: post
category: 分享
title:  ACL主席王海峰和百度巨牛们探讨NLP的挑战和未来(微信讲座文字版)
date:   2015-06-17 23:26:05
tagline: by WuBin 武斌
tags: [DeepLearning, weichat, BaiDu, NLP]
published: true
author: wubin

---
微信讲座-ACL主席王海峰和百度巨牛们探讨NLP的挑战和未来

<!--more-->

## 嘉宾：
1. 王海峰

	现任百度技术副总裁，负责百度搜索引擎技术和自然语言处理相关技术。同时，他还是自然语言处理领域国际学术组织ACL50年历史上唯一华人主席。已授权或公开的专利申请80余项，已发表学术论文90余篇。

2. 吴华

	百度技术委员会主席，机器翻译相关技术负责人。申请专利20余件、发表论文30余篇，曾任IJCNLP, ACL等国际学术会议机器翻译领域主席。

3. 贾磊

	百度语音处理相关技术的负责人。在百度用不到3年的时间，带领团队利用深度学习技术使百度语音技术跻身世界前列。

4. 赵世奇

	百度知识图谱相关技术负责人，智能机器人“百小度“的技术负责人。他是第一个进驻百度博士后工作站的博士，李彦宏亲自担任其导师。

## 内容

### 王海峰
 
来从人类语言处理的角度，综合讲一下当前的发展，挑战和未来，欢迎！

今天的主题是人类语言技术，包括自然语言处理技术和语音技术。

这方面的技术，人类已经研究了60多年。

近年来，随着互联网的高速发展，给这些领域带来了空前的机遇和挑战，推动了人类语言技术的高速发展，催生了大量技术及应用成果。

互联网为这些技术带来的包括如下几方面：应用需求、大数据、计算平台。

在各种互联网应用需求的牵引下，在强大的计算平台支撑下，基于远远超过历史上任何时候的互联网大数据，技术也得到了飞速的提升。很多技术都在过去几年达到了真正可以实用的水平。

同时，这些技术又不是孤立的。一方面，各种人类语言技术相互结合，例如语音与机器翻译结合，产出了实用的语音翻译。

另一方面，通过结合其它互联网技术，也大大促进促进了人类语言技术的进步。例如，得益于互联网搜索技术、大数据挖掘技术等，机器翻译得到了空前快速的进步，百度翻译每天响应的翻译请求量已经上亿。

我先说这些。下面，贾磊、吴华、赵世奇三位会介绍一下语音技术、机器翻译与深度问答、及知识图谱。

### 贾磊

谢谢大家，我是百度贾磊

目前负责百度内部的语音技术，包括语音识别，语音合成，说话人认证等。

语音识别技术：是把声音信号转换成文字的技术。其中的技术包含数字信号处理，声学深度模型，语言深度模型，解码器，以及结合语义对话的一些识别技术。

语音合成技术是把文字转化成声音的技术。就是让文字实现自动的声音播报。

说话人认证技术，就是根据你说话的声音，进行说话人的声纹认证

这个领域的最新进展包括海量语料的声学建模技术，海量语料的语言模型技术，结合语义理解的语音识别技术等。进展最大的就是深度学习在语音识别中的应用，其中包括基于DNN的声学建模，基于CNN的声学建模，基于LSTM的声学建模等。深度学习的高速训练也是目前最重要的技术方向之一。

语音识别的解码器技术进展也对现有技术有很大的促进作用，解码器需要融合语义，基于场景的语音识别对识别技术的促进作用非常大

语音识别技术面临的主要压力是方言，说话方式，口音和噪音。本质上这些问题几乎无法短期内完全根本解决。安静环境普通话识别率有希望逼近98%，但是噪音、口音和说话方式等因素确实对识别率的影响会长期存在。

个性化的语音识别技术和基于对话场景的语音识别技术将成为语音识别走向使用的关键。

我的介绍结束了

### 吴华

大家好，我是百度吴华。目前是百度自然语言处理技术负责人。

NLP技术在互联网上被广泛地应用，主要归结于互联网大数据和群体智慧、大规模并行计算的发展、算法本身的优化。

百度NLP部门研发的内容包括语义理解、深度问答、对话技术、机器翻译、机器学习等。

比如query语义理解获取用户的需求，深度问答技术直接精准地给出答案；对话技术实现和人聊天以及多轮问答需求，我们最近推出的小度机器人就综合利用了以上技术满足用户需求。

除了中文相关的技术，百度也做多语言技术，比如百度翻译满足用户多语言翻译需求，目前支持27个语种、702个翻译方向。

以上这些应用中，都有机器学习作为基础技术支持，比如在机器翻译中就用了最新的深度学习LSTM模型。

下面我简单介绍两个技术：深度问答和机器翻译

####1. 深度问答
在智能机器人应用中，其中一个关键技术就是深度问答。深度问答技术就是想让计算机能正确地理解人类的语言，并准确地给出答案，使机器拥有人类的智能。

最近几年，在知识问答方面首先取得突破的IBM Wastom开发的DeepQA系统，在2011年参加美国《危险边缘》（Jeopardy!）节目答题准确率战胜了人类选手，成为冠军。

这次的成功主要归功于互联网上大数据如Wikipedia知识的运用，以及在技术上的几个关键突破：

	1. 深层理解和浅层理解的结合

	2. 信息置信度的估计

		这个方法颠覆了研究人员对深度问答的认识，之前深度问答被认为必须在通用的本体知识的指导下，进行深层次的逻辑推理与演算才能成功。

然而，深度问答技术要想在通用领域使用，依然存在以下挑战：

	1. 深度问答依赖的技术较多，如自然语言处理、信息检索、知识表示和推理、机器学习等，容易出现错误传导现象，导致通用领域回答精度偏低；

	2. 由于通用语义理解技术水平的限制，从无结构化文本中提取结构化知识的准确率和覆盖率都不高，因此构建高质量结构化知识库难度很高

	3. 深度问答的推理是NLP老大难问题，目前主要采用规则的方法，但覆盖面和准确率都不高

因此，在技术上，需要新的学习和推理机制，目前有研究者利用Neural Memory等方法。

如Sukhbaatar et al. End-To-End Memory Networks

该方法能够针对某一类特定的问题进行推理并给出直接答案，希望能有更多的突破

在应用上，主要集中在基于知识图谱的实体问答，百度在非实体问答（比如孕妇能吃荔枝吗？）进行了首次尝试

应用主要集中在需求大而且明确的特定领域上，通用领域的深度问答技术还未大规模应用。

深度问答技术相关内容介绍完了
 
#### 2.机器翻译

接下来简单介绍第二个技术：机器翻译
 
机器翻译从1947年提出到现在有60多年的历史，出现过规则、实例和统计三种经典的机器翻译方法。
 
统计机器翻译方法具有良好的鲁棒性、开发周期短、翻译质量高的特点
 
因此目前占据了在线翻译系统的主导地位

但是该方法的缺点是没有考虑整句的语义信息，因此在译文的准确度以及流畅度方面一直差强人意

随着机器学习方法的不断改进，尤其是深度学习方法的出现，整句语义信息的建模出现了转机
 
通常采用的方法有卷积神经网络（Convolutional neural network）、循环神经网络（recurrent neural network）等
 
由于循环神经网络有长句vanishing gradient problem，因此也不能很好地建立长距离词的搭配关系
 
为了解决这个问题，出现了Long Short-Term Memory (LSTM)的RNN方法
 
目前应用在机器翻译中的深度学习模型就是基于LSTM的encoder-decoder模型
 
但是这个模型也有自身的缺陷
 
为使解码速度达到实时要求，词表有限制，影响了翻译质量
 
因此，研究者开始做传统统计机器翻译方法和深度学习方法的融合

以解决OOV问题以及漏词的问题，寻求翻译质量的进一步突破
 
未来希望能更多地整合语义理解的进展和机器学习的进展，生成更流畅的译文。
 
我的介绍完了。
 
### 赵世奇

大家好，我是赵世奇，在百度NLP从事知识挖掘和对话系统的研发工作。今天我主要和大家一起探讨知识图谱（knowledge graph）相关的技术。

简单概括，我们可以认为人是从数据中抽象出知识，再基于知识而产生智能。而在当今的大数据时代，学术界和产业界基于大数据开发智能系统并提供智能服务，这背后的基础仍然是知识，即从大数据中挖掘提炼出来的知识。
 
正因如此，知识图谱的重要性毋庸置疑。目前，百度、Google、Bing等搜索引擎都构建了自己的知识图谱，并在其基础上设计开发了多种产品和服务。
 
知识图谱的内涵很丰富，它包含了知识的采集、存储、计算和应用。知识图谱中的基本知识单元可以认为是“实体（entity）”，每个实体具有“属性（attribute）”和“值（value）”；而实体和实体之间则通过“关系（relation）”链接起来，从而构成一张图谱（graph）。

知识图谱中的知识可以来自互联网的每一个角落，比如结构化的表格，半结构化的网页数据块，甚至是无结构化的自然语言句子。知识图谱对于知识的准确率要求很高，通常都是要求在99%以上，这样才能保证在各种应用中向用户提供准确且有价值的知识。
 
为了保证知识的准确性，除了加强知识的清洗、过滤等技术之外，还可以通过引入“众包（crowdsourcing）”的机制，请普通用户来参与数据质量的优化。
 
知识图谱的应用形式多种多样。最简单的，知识图谱可以提供知识的查询和展现，比如对于一个电影明星，展现他/她的个人知识，供用户了解。
 
知识图谱还可以支持问答，例如对于类似“珠穆朗玛峰的高度”这样的问题，可以直接从知识图谱中检索到精准答案，甚至对于“身高超过185的男明星”这样的复杂query，也可以通过知识图谱查询到答案。
 
此外，知识图谱还可以应用于相关推荐，例如用户搜索一部电影，基于知识图谱推荐出该电影中的主要演员/角色，以及其他同题材的电影等。
 
除了“开放域”的知识图谱之外，还可以针对不同的“垂直领域”构建知识图谱，如在餐饮、医疗、教育、金融等领域。垂类的知识图谱可以支持很多深度的、有特色的服务。

例如，在教育垂类，通过构建一个围绕知识点的知识图谱，可以更好的帮助学生学习所需的知识点，并进而推荐其他相关的知识点，从而提升学生的学习效率。

以上是一个简要的介绍，期待和大家深入交流。谢谢

### 提问环节

1. @HUA 百度翻译所采用的LSTM网络有多大，一个模型搞定全部问题吗？

	百度翻译LSTM一层是千量级的节点，我们用的是end to end LSTM 模型。一个模型能完成翻译的全过程，但同时集成了传统统计翻译的技术。

2. @HUA CNN的特性是擅长二维信号，为何在一维的nlp上有用？rnn的梯度vanish不正好成为时间窗吗？

	CNN在二维图像上的做的相当于一种局部信息抽象整合的作用，NLP的句子是一维的序列也可算是二维的特例，也有局部窗口的概念，所以相对于一般的Bag-of-Words的NN是有提升作用的。但RNN较CNN对NLP来说更适配序列输入的特性，相对更合理一些，我们的实际效果来看好多任务上RNN也比CNN更好一些。

3. @赵世奇 知识图谱和本体有什么区别

	知识图谱和本体是有很多共同的概念的，比如实体、属性、关系等等。我认为本体更多的是哲学层面和理论层面的概念，但知识图谱则更多是互联网应用背景下的概念。同学们不必过多的强调二者的不同。

4. @赵世奇 ，目前工业上如何对短文本进行分类？

	不同的短文本方法会很不同，但是有一个共性是“巧妇难为无米之炊”。所以关键是如何找到其他的辅助信息。对于搜索引擎而言，最典型的短文本就是用户的搜索query。对于query的分类是方方面面的，其中一类很重要的分类是需求意图的分类。query意图分类的一种典型方法是基于搜索用户日志的点击关系。
	
	比如一个query点击的网站都是旅游相关的网站，那么这个query的意图很可能就是旅游方面的,也就是说，用点击的网站的内容（如title、snippet等）来扩充query，从而更准确的进行分类。
	
	另外还有一类比较典型的短文本是微博，往往也会基于用户之间的follow关系、以及回复转发评论等数据来扩充微博文本本身。
	
	因此，概括起来就是，对于短文本，想方设法的找到其他补充的内容，而不要只使用短文本本身。

5. @赵世奇 知识图谱的存储是使用什么数据库呢？如何实现知识图谱的高效查询呢？

	这个问题分两层来看：
	
	首先，对于我们挖掘的海量的知识，可以用关系数据库或者图数据库来存储，但同时需要注意的是，每个应用其实都只使用到知识图谱的一个局部，因此针对不同的应用，都会有知识图谱的一个子图。这个子图的规模往往不会太大，所以存储以及查询效率通常都不是太大的问题。
	
	另外，对于实际基于知识图谱的查询query，往往也都不会是复杂的query，通常不需要多层的查询。

6. 对词边界歧义问题，目前是采用哪些手段解决的 @HUA
我们针对不同的应用，采用不同的方法，比如采用CRF的模型+规则的方法

7. @HUA 百度翻译LSTM如何训练的
	
	在GPU上用SGD并行训练的

8. @HUA 推理机制是不是需要很强大的推理机，需要逻辑来进行支持？

	推理有多种，主要有基于知识和基于统计的方法，基于知识的推理机制中，需要有强大的知识库或者ontology支持，并在此基础上编写规则进行推理；基于统计的方法，主要是用机器学习的方法，可以不需要大型ontology的支持，但是在覆盖率上差强人意。

9. @贾磊老师，请问DNN,CNN,LSTM优缺点有哪些？

	在语音识别上，CNN一般在最底部，用于从原始raw singal中抽取生物的显著性特征，pooling有生物信息论上的强信号抽取的特点。DNN一般采用多层结构，是深度学习的核心实现，可以有很多层，用于误差扰动的消除和精确建模。LSTM会放在DNN的上面，用于学习轨迹特征，长时信息，可以有2-3等。他们需要结合使用，各有所长，各有各的价值。


10. @赵世奇请问下有没有系列学习知识图谱的资料？推荐引擎和搜索引擎该如何结合？有没有相关的文献？

	知识图谱是近些年新产生的概念，因此尚不存在系统性的学习材料。关于推荐引擎和搜索引擎结合的问题，百度搜索引擎就已经做了很多的尝试。同学们可以细心观察一下，搜索结果页左侧的内容主要是搜索结果，而右侧的很多内容都是推荐的内容，右侧推荐的主要作用在于延展用户的潜在需求。


11. @赵世奇 知识图谱中使用图算法多吗？主要用了那些？图计算基于什么平台？
	
	图算法在具体的工作中有很多不同的应用形态。可能难以一概而论。这里仅举两个例子：比如在基于query log的知识挖据工作中，经常会使用random walk，来进行同需求query的聚类；再比如在新词/新专名的挖掘过程中，我们会使用label propagation的算法来扩充新词、新专名。

12. 请问 您对神经网络做知识推理有什么看法？您是否看好基于知识图谱表示学习的知识推理？众包做知识清理的形式是怎样的，用到什么具体众包技术？

	基于深度学习做知识推理的工作的确有人在做，国内也有学者做相关的研究。比如基于已有的relation，推理学习出新的relation。但整体来说工作还不是特别多，我们还需要持续跟进了解。

	关于众包的问题，我们会面向不同的问题设计不同的众包任务，这中间最主要的问题是如何防止群体作弊，是一个很有趣的斗智斗勇的过程:)

13. @HUA 在机器翻译中，除了提高平行语料的质量外，在参数调优上有什么好的方法？

	在传统统计方法中，参数调优上我们主要用的经典的MERT方法，对于大型在线翻译，参数调优的方法区别不大，主要在模型的选择上很重要


14. @赵世奇 对于数据结构 比如结构型 半结构以及无结构在构建图谱时候是如何统一处理的？

	这三种不同来源的数据没有统一的处理方法，肯定是要分别挖掘的，在产出的知识层面再合并入库。如果说“统一”，也只是在最终知识的表达、存储等方面的统一。

15. 请问目前深度学习，包括word2vec，lstm等最新技术，对改善传统的query doc搜索的质量上有多大的实际帮助，主要体现在哪些方面呢，谢谢。@HUA 

	基于embedding方式语义表示的思想，可以进一步设计深度学习语义匹配模型。在大数据有监督训练下，可对query-doc的相关性计算有巨大帮助，目前我们在百度搜索上已经有成功的应用，显著提升了相关性效果。

	近期我们对LSTM改进基础上的新的model，在之前基础又有了效果的明显的提升。这类方法最明显的是克服了之前字面匹配的不足，具有很好的语义泛化作用，并且能够特别体现对用户满足角度来说相关/不相关的这种特定应用场景上的区分性。

	单纯的word2vec技术帮助不太大，但是这个思想很重要

16. @微讲座转发  赵世奇 知识图谱的数据量有多大？多久更新一次？每次只更新一小块？
	
	数量级大概是：亿级别的实体，十亿级别的属性；千亿级别的关系

	更新的周期不同的品类是不一样的
	
	时效性的类别可能更新很快，但有一些没有时效性需求的类别则并不需要经常更新



17. @吴华 您好，吴老师，我曾经在很多个问答机器人上提问过一个问题“我对海鲜过敏，能吃鱿鱼吗”，但基本上没有收到针对整句话的回答，能简单说明一下针对这类问题的实现思路吗？

	主要有以下几个步骤：
		
		（1）通过搜索引擎找到相关网页，并从中这些相关网页中定位出答案片段；
		（2）根据答案片段进行观点聚合；
		（3）对聚合的观点进行相关性排序和置信度评估，最后给出综合答案结果


18. @HUA 请问业界的训练数据的句子数量和单词数量大概在什么规模？请问在数据筛选方面有什么经验？RNN和LSTM对于长句子翻译的问题怎么解决？
	
	双语句对数在亿量级。数据筛选，既要用到分析、对齐等NLP技术，也要用到网页分析、站点权威性等其它互联网技术。
	
	RNN和LSTM是可以对长句子进行整体建模的，但是在实际应用中，我们会折中考虑性能开销酌情用不同的技术。


19. @赵世奇 请问kb上的qa目前业界做得怎么样？对于keyword query和句子的回答准确率能到多少？是通过什么方法把query转换成结构化查询的？
	
	对于这个问题，我回答一下最后这个问题，我们通过句子的深层语义分析将问句转化成结构化的语义表达，进而转化成知识库的查询语句


20. @吴华老师，深度理解和浅层理解具体指什么？
	
	深层理解主要指NLP领域的句法语义分析方法，浅层理解主要是用建立模板的方法

21. @HUA 我对qa系统不是很熟悉 想简单了解一下 从文本查询到三元组的解析过程
	
	第一步是对query进行语法语义分析，变成一个中间语义表示。根据这个语义表示，在知识库中匹配对应的三元组就完成了一个查询过程
